{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7bd7c4-4017-4fde-87c6-a81ca73986e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf0897f-bbb3-4e6a-93d2-bae3487d6fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n",
      "\n",
      "   Загрузка данных\n",
      "Train: 19028 записей\n",
      "Val: 2378 записей\n",
      "Test: 2379 записей\n",
      "Количество классов: 1200\n",
      "\n",
      "Используем batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True  # Автопоиск быстрых алгоритмов\n",
    "    torch.backends.cudnn.deterministic = False  # Для скорости, но немного жертвуем воспроизводимостью\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ArcFace с возможностью менять margin\n",
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=64.0, m=0.5, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = float(s)\n",
    "        self.m = float(m)\n",
    "        self.eps = eps\n",
    "\n",
    "        # Веса классификатора (центры классов)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        #Используем равномерное распределение, диапазон которого зависит от числа входящих и выходящих нейронов\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        # Вычисляем тригонометрические константы\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "\n",
    "    def set_margin(self, m: float):\n",
    "        #Изменяет коэффициент m и пересчитывает тригонометрические константы\n",
    "        self.m = float(m)\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "\n",
    "    def forward(self, embeddings, labels=None):\n",
    "        # Нормализуем эмбеддинги и веса\n",
    "        x = F.normalize(embeddings, p=2, dim=1)  # (B, D) - размер батча и размерность эмбеддинга, p=2 -> L2 нормализация\n",
    "        W = F.normalize(self.weight, p=2, dim=1)  # (N, D) - количество классов и размерность эмбеддинга\n",
    "        \n",
    "        # Вычисляем косинусы углов между эмбеддингами и центрами классов, clamp ограничивает снизу нулём, чтобы вдруг не произошло округления к отрицательному числу\n",
    "        cosine = F.linear(x, W)  # (B, N)\n",
    "        cosine = cosine.clamp(-1.0 + self.eps, 1.0 - self.eps)\n",
    "        \n",
    "        # Если меток нет (инференс), просто возвращает масштабированные косинусы\n",
    "        if labels is None:\n",
    "            return self.s * cosine\n",
    "        \n",
    "        # Для обучения вычисляем sin(theta)\n",
    "        sine = torch.sqrt(torch.clamp(1.0 - cosine * cosine, min=0.0) + self.eps)\n",
    "        \n",
    "        # Вычисляем cos(theta + m) используя тригонометрическую формулу косинуса суммы\n",
    "        cos_theta_m = cosine * self.cos_m - sine * self.sin_m\n",
    "        \n",
    "        # Создаем one-hot вектор для целевых классов\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1.0)\n",
    "        \n",
    "        # Формируем финальные логиты:\n",
    "        # Для целевого класса используем cos(theta + m)\n",
    "        # Для остальных классов используем cos(theta)\n",
    "        logits = cosine.clone()\n",
    "        logits = logits * (1.0 - one_hot) + cos_theta_m * one_hot\n",
    "        \n",
    "        # Масштабируем логиты\n",
    "        logits = logits * self.s\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class FaceModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        embedding_size=512,\n",
    "        use_arcface=False,\n",
    "        arc_s=64.0,\n",
    "        arc_m=0.55,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_arcface = bool(use_arcface)\n",
    "\n",
    "        weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        backbone = models.resnet34(weights=weights)\n",
    "        in_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Изначально все слои замораживаю - разморожу layers 3 и 4 на втором этапе обучения\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(in_features, embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        if self.use_arcface:\n",
    "            self.classifier = ArcFace(in_features=embedding_size, out_features=num_classes, s=arc_s, m=arc_m)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(embedding_size, num_classes)\n",
    "            # Тут используем такую же инициализацию весов, как в arcface\n",
    "            nn.init.xavier_uniform_(self.classifier.weight)\n",
    "            if self.classifier.bias is not None:\n",
    "                nn.init.constant_(self.classifier.bias, 0.0)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        feats = self.backbone(x)\n",
    "        embeddings = self.embedding(feats)\n",
    "        \n",
    "        if self.use_arcface:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            logits_or_cos = self.classifier(embeddings, labels)\n",
    "            return logits_or_cos, embeddings\n",
    "        else:\n",
    "            logits = self.classifier(embeddings)\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            return logits, embeddings\n",
    "\n",
    "# Подготовка данных\n",
    "print(\"\\n   Загрузка данных\")\n",
    "\n",
    "train_df = pd.read_csv(r'datasets/train_aligned.csv')\n",
    "val_df = pd.read_csv(r'datasets/val_aligned.csv')\n",
    "test_df = pd.read_csv(r'datasets/test_aligned.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)} записей\")\n",
    "print(f\"Val: {len(val_df)} записей\")\n",
    "print(f\"Test: {len(test_df)} записей\")\n",
    "\n",
    "# Поскольку id людей в отобранном датасете идут не по порядку, применяю Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([train_df['person'], val_df['person'], test_df['person']])\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "train_df['encoded_person'] = label_encoder.transform(train_df['person'])\n",
    "val_df['encoded_person'] = label_encoder.transform(val_df['person'])\n",
    "test_df['encoded_person'] = label_encoder.transform(test_df['person'])\n",
    "\n",
    "NUM_CLASSES = len(label_encoder.classes_)\n",
    "print(f\"Количество классов: {NUM_CLASSES}\")\n",
    "\n",
    "'''Миксап тоже представляет из себя технику аугментации, проводящую линейную интерполяцию между двумя изображениями и их метками.\n",
    "   Если простым языком - попиксельно соединяет две фотографии в одну, вместо конкретной метки [1, 0] тоже получаем [X, 1-X]'''\n",
    "def mixup_data(x, y, alpha=0.25):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    '''Зеркальное отражение хоть и является геометрической аугментацией, но расположение глаз всё таки не меняет, поэтому использую её.\n",
    "       Остальные аугментации, затрагивающие геометрию изображений, вероятно будут ухудшать результат - иначе зачем тогда на прошлом этапе мы\n",
    "       проводили выравнивание?'''\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    \n",
    "    A.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.15,\n",
    "        p=0.8\n",
    "    ),\n",
    "    A.ToGray(p=0.2),\n",
    "    A.GaussianBlur(blur_limit=(3, 3), p=0.33),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "        '''В попытке перестроить структуру проекта на более-менее внятно разложенные по папкам файлы, в прошлом задании\n",
    "           написал некорректные пути доступа к фотографиям. Иправить несложно конечно, но осадочек остался'''\n",
    "        self.data['path'] = self.data['path'].apply(lambda x: x[3:])\n",
    "        self.data['path'] = self.data['path'].replace(r'gavri/datasets', r'gavri/FRProject/datasets')\n",
    "        \n",
    "        self.paths = self.data['path'].values.tolist()\n",
    "        self.labels = self.data['encoded_person'].values.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.paths[idx]\n",
    "        \n",
    "        # OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            # Fallback\n",
    "            image = np.ones((112, 112, 3), dtype=np.uint8) * 255\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Применяем Albumentations трансформации\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_dataset = FaceDataset(train_df, train_transform)\n",
    "val_dataset = FaceDataset(val_df, val_transform)\n",
    "test_dataset = FaceDataset(test_df, val_transform)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32  # на практике оказалось самым рабочим размером\n",
    "\n",
    "\n",
    "print(f\"\\nИспользуем batch_size: {BATCH_SIZE}\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, use_mixup=False, mixup_alpha=0.25):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Если ArcFace - НЕ используем MixUp\n",
    "        if getattr(model, 'use_arcface', False):\n",
    "            logits, _ = model(images, labels)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Если обычный классификатор + включен MixUp\n",
    "        elif use_mixup and np.random.random() < 0.5:\n",
    "            mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=mixup_alpha)\n",
    "            logits, _ = model(mixed_images)\n",
    "            \n",
    "            # Смешанный лосс для MixUp\n",
    "            loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "            \n",
    "            # Смешанная accuracy (для мониторинга)\n",
    "            with torch.no_grad():\n",
    "                _, predicted = logits.max(1)\n",
    "                total += labels.size(0)\n",
    "                # Weighted accuracy для MixUp\n",
    "                correct_mixup = lam * (predicted == labels_a).float() + (1 - lam) * (predicted == labels_b).float()\n",
    "                correct += correct_mixup.sum().item()\n",
    "        \n",
    "        # Обычный режим (без MixUp)\n",
    "        else:\n",
    "            logits, _ = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits, _ = model(images)  # labels=None - arcface вернет S*cos без маржи (не будет усложнять, как он это делает на обучении)\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bbeee-5f0e-40c9-b1b5-a91eab89c010",
   "metadata": {},
   "source": [
    "Обучение с CE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6545784d-179b-4bd5-9d06-66e2ed04004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Создание модели ResNet34 (без ArcFace)\n",
      "\n",
      "Этап 1: Только head (15 эпох)\n",
      "\n",
      "Эпоха 1/15\n",
      "Train: loss=7.0013, acc=0.0169\n",
      "Val: acc=0.0429\n",
      "Сохранена лучшая модель (val_acc=0.0429)\n",
      "\n",
      "Эпоха 2/15\n",
      "Train: loss=6.3799, acc=0.0734\n",
      "Val: acc=0.0887\n",
      "Сохранена лучшая модель (val_acc=0.0887)\n",
      "\n",
      "Эпоха 3/15\n",
      "Train: loss=6.1109, acc=0.1276\n",
      "Val: acc=0.1253\n",
      "Сохранена лучшая модель (val_acc=0.1253)\n",
      "\n",
      "Эпоха 4/15\n",
      "Train: loss=5.9291, acc=0.1713\n",
      "Val: acc=0.1417\n",
      "Сохранена лучшая модель (val_acc=0.1417)\n",
      "\n",
      "Эпоха 5/15\n",
      "Train: loss=5.8126, acc=0.2075\n",
      "Val: acc=0.1451\n",
      "Сохранена лучшая модель (val_acc=0.1451)\n",
      "\n",
      "Эпоха 6/15\n",
      "Train: loss=5.9308, acc=0.1674\n",
      "Val: acc=0.1333\n",
      "\n",
      "Эпоха 7/15\n",
      "Train: loss=5.8213, acc=0.1944\n",
      "Val: acc=0.1556\n",
      "Сохранена лучшая модель (val_acc=0.1556)\n",
      "\n",
      "Эпоха 8/15\n",
      "Train: loss=5.7617, acc=0.2116\n",
      "Val: acc=0.1699\n",
      "Сохранена лучшая модель (val_acc=0.1699)\n",
      "\n",
      "Эпоха 9/15\n",
      "Train: loss=5.6374, acc=0.2458\n",
      "Val: acc=0.1901\n",
      "Сохранена лучшая модель (val_acc=0.1901)\n",
      "\n",
      "Эпоха 10/15\n",
      "Train: loss=5.5244, acc=0.2786\n",
      "Val: acc=0.1892\n",
      "\n",
      "Эпоха 11/15\n",
      "Train: loss=5.4545, acc=0.2975\n",
      "Val: acc=0.2052\n",
      "Сохранена лучшая модель (val_acc=0.2052)\n",
      "\n",
      "Эпоха 12/15\n",
      "Train: loss=5.3880, acc=0.3231\n",
      "Val: acc=0.2183\n",
      "Сохранена лучшая модель (val_acc=0.2183)\n",
      "\n",
      "Эпоха 13/15\n",
      "Train: loss=5.3397, acc=0.3381\n",
      "Val: acc=0.2094\n",
      "\n",
      "Эпоха 14/15\n",
      "Train: loss=5.2671, acc=0.3615\n",
      "Val: acc=0.2187\n",
      "Сохранена лучшая модель (val_acc=0.2187)\n",
      "\n",
      "Эпоха 15/15\n",
      "Train: loss=5.2560, acc=0.3691\n",
      "Val: acc=0.2161\n",
      "\n",
      "Этап 2: Размораживаем весь backbone (35 эпох)\n",
      "\n",
      "Эпоха 1/35\n",
      "Train: loss=5.5940, acc=0.2522\n",
      "Val: acc=0.3402\n",
      "Сохранена лучшая модель (val_acc=0.3402)\n",
      "\n",
      "Эпоха 2/35\n",
      "Train: loss=4.9814, acc=0.4619\n",
      "Val: acc=0.4407\n",
      "Сохранена лучшая модель (val_acc=0.4407)\n",
      "\n",
      "Эпоха 3/35\n",
      "Train: loss=4.6631, acc=0.5755\n",
      "Val: acc=0.5147\n",
      "Сохранена лучшая модель (val_acc=0.5147)\n",
      "\n",
      "Эпоха 4/35\n",
      "Train: loss=4.4479, acc=0.6634\n",
      "Val: acc=0.5332\n",
      "Сохранена лучшая модель (val_acc=0.5332)\n",
      "\n",
      "Эпоха 5/35\n",
      "Train: loss=4.3211, acc=0.7137\n",
      "Val: acc=0.5761\n",
      "Сохранена лучшая модель (val_acc=0.5761)\n",
      "\n",
      "Эпоха 6/35\n",
      "Train: loss=4.1619, acc=0.7685\n",
      "Val: acc=0.6127\n",
      "Сохранена лучшая модель (val_acc=0.6127)\n",
      "\n",
      "Эпоха 7/35\n",
      "Train: loss=4.0724, acc=0.7986\n",
      "Val: acc=0.6510\n",
      "Сохранена лучшая модель (val_acc=0.6510)\n",
      "\n",
      "Эпоха 8/35\n",
      "Train: loss=3.9266, acc=0.8446\n",
      "Val: acc=0.6611\n",
      "Сохранена лучшая модель (val_acc=0.6611)\n",
      "\n",
      "Эпоха 9/35\n",
      "Train: loss=3.9127, acc=0.8471\n",
      "Val: acc=0.6690\n",
      "Сохранена лучшая модель (val_acc=0.6690)\n",
      "\n",
      "Эпоха 10/35\n",
      "Train: loss=3.7805, acc=0.8813\n",
      "Val: acc=0.6997\n",
      "Сохранена лучшая модель (val_acc=0.6997)\n",
      "\n",
      "Эпоха 11/35\n",
      "Train: loss=3.7104, acc=0.8980\n",
      "Val: acc=0.6976\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 12/35\n",
      "Train: loss=3.6818, acc=0.8988\n",
      "Val: acc=0.7140\n",
      "Сохранена лучшая модель (val_acc=0.7140)\n",
      "\n",
      "Эпоха 13/35\n",
      "Train: loss=3.6679, acc=0.9007\n",
      "Val: acc=0.7208\n",
      "Сохранена лучшая модель (val_acc=0.7208)\n",
      "\n",
      "Эпоха 14/35\n",
      "Train: loss=3.6179, acc=0.9081\n",
      "Val: acc=0.7241\n",
      "Сохранена лучшая модель (val_acc=0.7241)\n",
      "\n",
      "Эпоха 15/35\n",
      "Train: loss=3.5211, acc=0.9314\n",
      "Val: acc=0.7380\n",
      "Сохранена лучшая модель (val_acc=0.7380)\n",
      "\n",
      "Эпоха 16/35\n",
      "Train: loss=3.5633, acc=0.9174\n",
      "Val: acc=0.7334\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 17/35\n",
      "Train: loss=3.5483, acc=0.9172\n",
      "Val: acc=0.7515\n",
      "Сохранена лучшая модель (val_acc=0.7515)\n",
      "\n",
      "Эпоха 18/35\n",
      "Train: loss=3.4858, acc=0.9295\n",
      "Val: acc=0.7431\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 19/35\n",
      "Train: loss=3.5187, acc=0.9128\n",
      "Val: acc=0.7603\n",
      "Сохранена лучшая модель (val_acc=0.7603)\n",
      "\n",
      "Эпоха 20/35\n",
      "Train: loss=3.4847, acc=0.9213\n",
      "Val: acc=0.7649\n",
      "Сохранена лучшая модель (val_acc=0.7649)\n",
      "\n",
      "Эпоха 21/35\n",
      "Train: loss=3.4975, acc=0.9151\n",
      "Val: acc=0.7683\n",
      "Сохранена лучшая модель (val_acc=0.7683)\n",
      "\n",
      "Эпоха 22/35\n",
      "Train: loss=3.4156, acc=0.9335\n",
      "Val: acc=0.7746\n",
      "Сохранена лучшая модель (val_acc=0.7746)\n",
      "\n",
      "Эпоха 23/35\n",
      "Train: loss=3.4244, acc=0.9266\n",
      "Val: acc=0.7742\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 24/35\n",
      "Train: loss=3.3583, acc=0.9387\n",
      "Val: acc=0.7839\n",
      "Сохранена лучшая модель (val_acc=0.7839)\n",
      "\n",
      "Эпоха 25/35\n",
      "Train: loss=3.4578, acc=0.9112\n",
      "Val: acc=0.7876\n",
      "Сохранена лучшая модель (val_acc=0.7876)\n",
      "\n",
      "Эпоха 26/35\n",
      "Train: loss=3.3550, acc=0.9357\n",
      "Val: acc=0.7864\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 27/35\n",
      "Train: loss=3.3946, acc=0.9255\n",
      "Val: acc=0.7881\n",
      "Сохранена лучшая модель (val_acc=0.7881)\n",
      "\n",
      "Эпоха 28/35\n",
      "Train: loss=3.3512, acc=0.9334\n",
      "Val: acc=0.7944\n",
      "Сохранена лучшая модель (val_acc=0.7944)\n",
      "\n",
      "Эпоха 29/35\n",
      "Train: loss=3.3681, acc=0.9290\n",
      "Val: acc=0.7910\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 30/35\n",
      "Train: loss=3.3751, acc=0.9257\n",
      "Val: acc=0.7931\n",
      "  Patience: 2/7\n",
      "\n",
      "Эпоха 31/35\n",
      "Train: loss=3.3484, acc=0.9319\n",
      "Val: acc=0.7981\n",
      "Сохранена лучшая модель (val_acc=0.7981)\n",
      "\n",
      "Эпоха 32/35\n",
      "Train: loss=3.3740, acc=0.9258\n",
      "Val: acc=0.8015\n",
      "Сохранена лучшая модель (val_acc=0.8015)\n",
      "\n",
      "Эпоха 33/35\n",
      "Train: loss=3.3415, acc=0.9334\n",
      "Val: acc=0.7965\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 34/35\n",
      "Train: loss=3.3466, acc=0.9300\n",
      "Val: acc=0.7986\n",
      "  Patience: 2/7\n",
      "\n",
      "Эпоха 35/35\n",
      "Train: loss=3.3056, acc=0.9387\n",
      "Val: acc=0.7977\n",
      "  Patience: 3/7\n",
      "\n",
      "Финальный fine-tune\n",
      "\n",
      "Эпоха 1/15\n",
      "Train: loss=3.3254, acc=0.9343\n",
      "Val: acc=0.8003\n",
      "  Patience: 1/5\n",
      "\n",
      "Эпоха 2/15\n",
      "Train: loss=3.3188, acc=0.9358\n",
      "Val: acc=0.7986\n",
      "  Patience: 2/5\n",
      "\n",
      "Эпоха 3/15\n",
      "Train: loss=3.2883, acc=0.9434\n",
      "Val: acc=0.7973\n",
      "  Patience: 3/5\n",
      "\n",
      "Эпоха 4/15\n",
      "Train: loss=3.3343, acc=0.9311\n",
      "Val: acc=0.7969\n",
      "  Patience: 4/5\n",
      "\n",
      "Эпоха 5/15\n",
      "Train: loss=3.3347, acc=0.9314\n",
      "Val: acc=0.7956\n",
      "  Patience: 5/5\n",
      "Ранняя остановка finetune\n",
      "\n",
      "Лучшая точность на валидации (best_val_acc): 0.8015\n",
      "\n",
      "   Загрузка лучшей модели\n",
      "Загружена лучшая модель из weights\\best_final_ce.pth\n",
      "  Val accuracy (saved): 0.7309\n",
      "  Train accuracy (saved): 0.9994\n",
      "\n",
      "   Тестирование\n",
      "Точность на тестовом наборе: 0.7503\n",
      "\n",
      "   Сохранение финальной модели\n",
      "Финальная модель сохранена в 'weights\\final_ce_model.pth'\n",
      "\n",
      "============================================================\n",
      "ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\n",
      "============================================================\n",
      "Лучшая val accuracy: 0.8015\n",
      "Test accuracy: 0.7503\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСоздание модели ResNet34 (без ArcFace)\")\n",
    "model = FaceModel(num_classes=NUM_CLASSES, embedding_size=512, use_arcface=False).to(device)\n",
    "\n",
    "#label_smoothing=0.3 помогает модели меньше перобучаться за счет небольшого распределения уверенности модели на другие классы\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_checkpoint = None\n",
    "\n",
    "\n",
    "print(\"\\nЭтап 1: Только head (15 эпох)\")\n",
    "\n",
    "for p in model.embedding.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer_stage1 = optim.AdamW(\n",
    "    [\n",
    "        {'params': model.embedding.parameters(), 'lr': 8e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 8e-4}\n",
    "    ],\n",
    "    weight_decay=0.03\n",
    ")\n",
    "\n",
    "scheduler_stage1 = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer_stage1, \n",
    "    T_0=5,           # Первый цикл - 5 эпох\n",
    "    T_mult=2,        # Удваиваем длину каждого следующего цикла\n",
    "    eta_min=1e-6     # Минимальный LR\n",
    ")\n",
    "\n",
    "EPOCHS_STAGE1 = 15\n",
    "for epoch in range(EPOCHS_STAGE1):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{EPOCHS_STAGE1}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer_stage1, criterion, use_mixup=True)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    scheduler_stage1.step()\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val: acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_checkpoint = {\n",
    "            'stage': 'stage1',\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_checkpoint, r'weights\\best_stage1_ce.pth')\n",
    "        print(f\"Сохранена лучшая модель (val_acc={val_acc:.4f})\")\n",
    "\n",
    "\n",
    "print(\"\\nЭтап 2: Размораживаем весь backbone (35 эпох)\")\n",
    "\n",
    "for param in model.backbone.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.backbone.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer_stage2 = optim.AdamW(\n",
    "    [\n",
    "        {'params': model.backbone.layer3.parameters(), 'lr': 2e-4},\n",
    "        {'params': model.backbone.layer4.parameters(), 'lr': 3e-4},\n",
    "        {'params': model.embedding.parameters(), 'lr': 4e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 4e-4}\n",
    "    ],\n",
    "    weight_decay=0.05\n",
    ")\n",
    "\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "EPOCHS_STAGE2 = 35\n",
    "\n",
    "scheduler_stage2 = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_stage2,\n",
    "    T_max=EPOCHS_STAGE2,  # 35 эпох плавного снижения\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE2):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{EPOCHS_STAGE2}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer_stage2, criterion, use_mixup=True)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    scheduler_stage2.step()\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val: acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_checkpoint = {\n",
    "            'stage': 'stage2',\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_checkpoint, r'weights\\best_stage2_ce.pth')\n",
    "        print(f\"Сохранена лучшая модель (val_acc={val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Ранняя остановка stage2\")\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"\\nФинальный fine-tune\")\n",
    "optimizer_finetune = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=5e-6,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "scheduler_finetune = optim.lr_scheduler.ReduceLROnPlateau(optimizer_finetune, mode='max', factor=0.5, patience=1)\n",
    "\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "EPOCHS_FINETUNE = 15\n",
    "\n",
    "for epoch in range(EPOCHS_FINETUNE):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{EPOCHS_FINETUNE}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer_finetune, criterion, use_mixup=True)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    scheduler_finetune.step(val_acc)\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val: acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_checkpoint = {\n",
    "            'stage': 'finetune',\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_checkpoint, r'weights\\best_final_ce.pth')\n",
    "        print(f\"Сохранена лучшая модель (val_acc={val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Ранняя остановка finetune\")\n",
    "            break\n",
    "            \n",
    "print(f\"\\nЛучшая точность на валидации (best_val_acc): {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n   Загрузка лучшей модели\")\n",
    "best_model_path = None\n",
    "if os.path.exists(r'weights\\best_final_ce.pth'):\n",
    "    best_model_path = r'weights\\best_final_ce.pth'\n",
    "elif os.path.exists(r'weights\\best_stage2_ce.pth'):\n",
    "    best_model_path = r'weights\\best_stage2_ce.pth'\n",
    "\n",
    "checkpoint = torch.load(best_model_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Загружена лучшая модель из {best_model_path}\")\n",
    "print(f\"  Val accuracy (saved): {checkpoint.get('val_acc', 0):.4f}\")\n",
    "print(f\"  Train accuracy (saved): {checkpoint.get('train_acc', 0):.4f}\")\n",
    "\n",
    "print(\"\\n   Тестирование\")\n",
    "test_acc = validate(model, test_loader)\n",
    "print(f\"Точность на тестовом наборе: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n   Сохранение финальной модели\")\n",
    "final_checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'train_acc': best_checkpoint.get('train_acc', 0) if best_checkpoint is not None else 0,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'model_type': 'FaceModel_CE'\n",
    "}\n",
    "torch.save(final_checkpoint, r'weights\\final_ce_model.pth')\n",
    "print(r\"Финальная модель сохранена в 'weights\\final_ce_model.pth'\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print('='*60)\n",
    "print(f\"Лучшая val accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8439bf-c455-4dac-bd7f-f9b93e495263",
   "metadata": {},
   "source": [
    "Обучение с ArcFaceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56258979-89d3-4d41-a745-b582ee315b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Создание модели ResNet34 (с ArcFace)\n",
      "\n",
      "Этап 1: Только head (15 эпох)\n",
      "\n",
      "Эпоха 1/15\n",
      "  m изменен на: 0.0000\n",
      "Train: loss=8.0995, acc=0.0152\n",
      "Val: acc=0.0547\n",
      "Сохранена лучшая модель (val_acc=0.0547)\n",
      "\n",
      "Эпоха 2/15\n",
      "  m изменен на: 0.0250\n",
      "Train: loss=7.1664, acc=0.0223\n",
      "Val: acc=0.1001\n",
      "Сохранена лучшая модель (val_acc=0.1001)\n",
      "\n",
      "Эпоха 3/15\n",
      "  m изменен на: 0.0500\n",
      "Train: loss=7.7232, acc=0.0125\n",
      "Val: acc=0.1434\n",
      "Сохранена лучшая модель (val_acc=0.1434)\n",
      "\n",
      "Эпоха 4/15\n",
      "  m изменен на: 0.0750\n",
      "Train: loss=8.6002, acc=0.0063\n",
      "Val: acc=0.1770\n",
      "Сохранена лучшая модель (val_acc=0.1770)\n",
      "\n",
      "Эпоха 5/15\n",
      "  m изменен на: 0.1000\n",
      "Train: loss=9.7070, acc=0.0019\n",
      "Val: acc=0.1808\n",
      "Сохранена лучшая модель (val_acc=0.1808)\n",
      "\n",
      "Эпоха 6/15\n",
      "  m изменен на: 0.1250\n",
      "Train: loss=11.7216, acc=0.0001\n",
      "Val: acc=0.1632\n",
      "\n",
      "Эпоха 7/15\n",
      "  m изменен на: 0.1500\n",
      "Train: loss=12.9627, acc=0.0004\n",
      "Val: acc=0.1678\n",
      "\n",
      "Эпоха 8/15\n",
      "  m изменен на: 0.1750\n",
      "Train: loss=14.1199, acc=0.0001\n",
      "Val: acc=0.1918\n",
      "Сохранена лучшая модель (val_acc=0.1918)\n",
      "\n",
      "Эпоха 9/15\n",
      "  m изменен на: 0.2000\n",
      "Train: loss=15.2864, acc=0.0000\n",
      "Val: acc=0.2082\n",
      "Сохранена лучшая модель (val_acc=0.2082)\n",
      "\n",
      "Эпоха 10/15\n",
      "  m изменен на: 0.2250\n",
      "Train: loss=16.4415, acc=0.0000\n",
      "Val: acc=0.2166\n",
      "Сохранена лучшая модель (val_acc=0.2166)\n",
      "\n",
      "Эпоха 11/15\n",
      "  m изменен на: 0.2500\n",
      "Train: loss=17.6013, acc=0.0000\n",
      "Val: acc=0.2225\n",
      "Сохранена лучшая модель (val_acc=0.2225)\n",
      "\n",
      "Эпоха 12/15\n",
      "  m изменен на: 0.2750\n",
      "Train: loss=18.7603, acc=0.0000\n",
      "Val: acc=0.2271\n",
      "Сохранена лучшая модель (val_acc=0.2271)\n",
      "\n",
      "Эпоха 13/15\n",
      "  m изменен на: 0.3000\n",
      "Train: loss=19.9164, acc=0.0000\n",
      "Val: acc=0.2372\n",
      "Сохранена лучшая модель (val_acc=0.2372)\n",
      "\n",
      "Эпоха 14/15\n",
      "  m изменен на: 0.3250\n",
      "Train: loss=21.1122, acc=0.0000\n",
      "Val: acc=0.2389\n",
      "Сохранена лучшая модель (val_acc=0.2389)\n",
      "\n",
      "Эпоха 15/15\n",
      "  m изменен на: 0.3500\n",
      "Train: loss=22.3332, acc=0.0000\n",
      "Val: acc=0.2426\n",
      "Сохранена лучшая модель (val_acc=0.2426)\n",
      "\n",
      "Этап 2: Размораживаем весь backbone (35 эпох)\n",
      "\n",
      "Эпоха 1/35\n",
      "  Margin m set to: 0.1000\n",
      "Train: loss=9.0754, acc=0.0049\n",
      "Val: acc=0.3322\n",
      "Сохранена лучшая модель (val_acc=0.3322)\n",
      "\n",
      "Эпоха 2/35\n",
      "  Margin m set to: 0.1300\n",
      "Train: loss=9.2050, acc=0.0060\n",
      "Val: acc=0.4777\n",
      "Сохранена лучшая модель (val_acc=0.4777)\n",
      "\n",
      "Эпоха 3/35\n",
      "  Margin m set to: 0.1600\n",
      "Train: loss=9.8960, acc=0.0054\n",
      "Val: acc=0.5505\n",
      "Сохранена лучшая модель (val_acc=0.5505)\n",
      "\n",
      "Эпоха 4/35\n",
      "  Margin m set to: 0.1900\n",
      "Train: loss=10.6486, acc=0.0054\n",
      "Val: acc=0.6081\n",
      "Сохранена лучшая модель (val_acc=0.6081)\n",
      "\n",
      "Эпоха 5/35\n",
      "  Margin m set to: 0.2200\n",
      "Train: loss=11.4568, acc=0.0061\n",
      "Val: acc=0.6619\n",
      "Сохранена лучшая модель (val_acc=0.6619)\n",
      "\n",
      "Эпоха 6/35\n",
      "  Margin m set to: 0.2500\n",
      "Train: loss=12.3095, acc=0.0067\n",
      "Val: acc=0.6876\n",
      "Сохранена лучшая модель (val_acc=0.6876)\n",
      "\n",
      "Эпоха 7/35\n",
      "  Margin m set to: 0.2800\n",
      "Train: loss=13.1489, acc=0.0071\n",
      "Val: acc=0.7002\n",
      "Сохранена лучшая модель (val_acc=0.7002)\n",
      "\n",
      "Эпоха 8/35\n",
      "  Margin m set to: 0.3100\n",
      "Train: loss=13.9306, acc=0.0095\n",
      "Val: acc=0.7124\n",
      "Сохранена лучшая модель (val_acc=0.7124)\n",
      "\n",
      "Эпоха 9/35\n",
      "  Margin m set to: 0.3400\n",
      "Train: loss=14.7300, acc=0.0101\n",
      "Val: acc=0.7267\n",
      "Сохранена лучшая модель (val_acc=0.7267)\n",
      "\n",
      "Эпоха 10/35\n",
      "  Margin m set to: 0.3700\n",
      "Train: loss=15.4533, acc=0.0109\n",
      "Val: acc=0.7321\n",
      "Сохранена лучшая модель (val_acc=0.7321)\n",
      "\n",
      "Эпоха 11/35\n",
      "  Margin m set to: 0.4000\n",
      "Train: loss=16.1637, acc=0.0118\n",
      "Val: acc=0.7380\n",
      "Сохранена лучшая модель (val_acc=0.7380)\n",
      "\n",
      "Эпоха 12/35\n",
      "  Margin m set to: 0.4300\n",
      "Train: loss=16.8176, acc=0.0130\n",
      "Val: acc=0.7452\n",
      "Сохранена лучшая модель (val_acc=0.7452)\n",
      "\n",
      "Эпоха 13/35\n",
      "  Margin m set to: 0.4600\n",
      "Train: loss=17.4145, acc=0.0134\n",
      "Val: acc=0.7489\n",
      "Сохранена лучшая модель (val_acc=0.7489)\n",
      "\n",
      "Эпоха 14/35\n",
      "  Margin m set to: 0.4900\n",
      "Train: loss=18.0080, acc=0.0125\n",
      "Val: acc=0.7532\n",
      "Сохранена лучшая модель (val_acc=0.7532)\n",
      "\n",
      "Эпоха 15/35\n",
      "  Margin m set to: 0.5200\n",
      "Train: loss=18.5813, acc=0.0129\n",
      "Val: acc=0.7574\n",
      "Сохранена лучшая модель (val_acc=0.7574)\n",
      "\n",
      "Эпоха 16/35\n",
      "  Margin m set to: 0.5500\n",
      "Train: loss=19.0355, acc=0.0104\n",
      "Val: acc=0.7565\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 17/35\n",
      "Train: loss=17.7723, acc=0.0254\n",
      "Val: acc=0.7611\n",
      "Сохранена лучшая модель (val_acc=0.7611)\n",
      "\n",
      "Эпоха 18/35\n",
      "Train: loss=16.4827, acc=0.0464\n",
      "Val: acc=0.7679\n",
      "Сохранена лучшая модель (val_acc=0.7679)\n",
      "\n",
      "Эпоха 19/35\n",
      "Train: loss=15.1544, acc=0.0775\n",
      "Val: acc=0.7582\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 20/35\n",
      "Train: loss=13.8664, acc=0.1128\n",
      "Val: acc=0.7628\n",
      "  Patience: 2/7\n",
      "\n",
      "Эпоха 21/35\n",
      "Train: loss=12.6062, acc=0.1557\n",
      "Val: acc=0.7641\n",
      "  Patience: 3/7\n",
      "\n",
      "Эпоха 22/35\n",
      "Train: loss=11.4610, acc=0.1967\n",
      "Val: acc=0.7662\n",
      "  Patience: 4/7\n",
      "\n",
      "Эпоха 23/35\n",
      "Train: loss=10.4608, acc=0.2394\n",
      "Val: acc=0.7725\n",
      "Сохранена лучшая модель (val_acc=0.7725)\n",
      "\n",
      "Эпоха 24/35\n",
      "Train: loss=9.5682, acc=0.2851\n",
      "Val: acc=0.7805\n",
      "Сохранена лучшая модель (val_acc=0.7805)\n",
      "\n",
      "Эпоха 25/35\n",
      "Train: loss=8.8387, acc=0.3204\n",
      "Val: acc=0.7750\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 26/35\n",
      "Train: loss=8.1506, acc=0.3656\n",
      "Val: acc=0.7712\n",
      "  Patience: 2/7\n",
      "\n",
      "Эпоха 27/35\n",
      "Train: loss=7.5926, acc=0.4016\n",
      "Val: acc=0.7717\n",
      "  Patience: 3/7\n",
      "\n",
      "Эпоха 28/35\n",
      "Train: loss=7.1182, acc=0.4290\n",
      "Val: acc=0.7742\n",
      "  Patience: 4/7\n",
      "\n",
      "Эпоха 29/35\n",
      "Train: loss=6.7539, acc=0.4580\n",
      "Val: acc=0.7805\n",
      "  Patience: 5/7\n",
      "\n",
      "Эпоха 30/35\n",
      "Train: loss=6.4879, acc=0.4695\n",
      "Val: acc=0.7860\n",
      "Сохранена лучшая модель (val_acc=0.7860)\n",
      "\n",
      "Эпоха 31/35\n",
      "Train: loss=6.2869, acc=0.4867\n",
      "Val: acc=0.7809\n",
      "  Patience: 1/7\n",
      "\n",
      "Эпоха 32/35\n",
      "Train: loss=6.0810, acc=0.4974\n",
      "Val: acc=0.7805\n",
      "  Patience: 2/7\n",
      "\n",
      "Эпоха 33/35\n",
      "Train: loss=6.0172, acc=0.5101\n",
      "Val: acc=0.7809\n",
      "  Patience: 3/7\n",
      "\n",
      "Эпоха 34/35\n",
      "Train: loss=5.9977, acc=0.5111\n",
      "Val: acc=0.7851\n",
      "  Patience: 4/7\n",
      "\n",
      "Эпоха 35/35\n",
      "Train: loss=5.9378, acc=0.5097\n",
      "Val: acc=0.7830\n",
      "  Patience: 5/7\n",
      "\n",
      "Финальный fine-tune\n",
      "m изменён на: 0.5000\n",
      "\n",
      "Эпоха 1/15\n",
      "Train: loss=4.3809, acc=0.6651\n",
      "Val: acc=0.7839\n",
      "  Patience: 1/5\n",
      "\n",
      "Эпоха 2/15\n",
      "Train: loss=4.3707, acc=0.6573\n",
      "Val: acc=0.7868\n",
      "Сохранена лучшая модель (val_acc=0.7868)\n",
      "\n",
      "Эпоха 3/15\n",
      "Train: loss=4.2890, acc=0.6588\n",
      "Val: acc=0.7847\n",
      "  Patience: 1/5\n",
      "\n",
      "Эпоха 4/15\n",
      "Train: loss=4.2679, acc=0.6591\n",
      "Val: acc=0.7834\n",
      "  Patience: 2/5\n",
      "\n",
      "Эпоха 5/15\n",
      "Train: loss=4.2301, acc=0.6619\n",
      "Val: acc=0.7822\n",
      "  Patience: 3/5\n",
      "\n",
      "Эпоха 6/15\n",
      "Train: loss=4.1902, acc=0.6641\n",
      "Val: acc=0.7826\n",
      "  Patience: 4/5\n",
      "\n",
      "Эпоха 7/15\n",
      "Train: loss=4.2066, acc=0.6593\n",
      "Val: acc=0.7809\n",
      "  Patience: 5/5\n",
      "Ранняя остановка finetune\n",
      "\n",
      "Лучшая точность на валидации (best_val_acc): 0.7868\n",
      "\n",
      "   Загрузка лучшей модели\n",
      "Загружена лучшая модель из weights\\best_final_arc.pth\n",
      "  Val accuracy (saved): 0.7868\n",
      "  Train accuracy (saved): 0.6573\n",
      "\n",
      "   Тестирование\n",
      "Точность на тестовом наборе: 0.7945\n",
      "\n",
      "   Сохранение финальной модели\n",
      "Финальная модель сохранена в 'weights\\final_arc_model.pth'\n",
      "\n",
      "============================================================\n",
      "ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\n",
      "============================================================\n",
      "Лучшая val accuracy: 0.7868\n",
      "Test accuracy: 0.7945\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСоздание модели ResNet34 (с ArcFace)\")\n",
    "model = FaceModel(num_classes=NUM_CLASSES, embedding_size=512, use_arcface=True, arc_s=64.0, arc_m=0.0).to(device)\n",
    "\n",
    "# При ArcFace НЕ используем label_smoothing\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_checkpoint = None\n",
    "\n",
    "print(\"\\nЭтап 1: Только head (15 эпох)\")\n",
    "\n",
    "for p in model.embedding.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer_stage1 = optim.AdamW(\n",
    "    [\n",
    "        {'params': model.embedding.parameters(), 'lr': 8e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 8e-4}\n",
    "    ],\n",
    "    weight_decay=0.03\n",
    ")\n",
    "\n",
    "scheduler_stage1 = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer_stage1, \n",
    "    T_0=5,           # Первый цикл - 5 эпох\n",
    "    T_mult=2,        # Удваиваем длину каждого следующего цикла\n",
    "    eta_min=1e-6     # Минимальный LR\n",
    ")\n",
    "\n",
    "EPOCHS_STAGE1 = 15\n",
    "for epoch in range(EPOCHS_STAGE1):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{EPOCHS_STAGE1}\")\n",
    "    current_margin = 0.35 * (epoch / (EPOCHS_STAGE1 - 1)) if EPOCHS_STAGE1 > 1 else 0.0\n",
    "    model.classifier.set_margin(current_margin)\n",
    "    print(f\"  m изменен на: {current_margin:.4f}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer_stage1, criterion, use_mixup=False)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    scheduler_stage1.step()\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val: acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_checkpoint = {\n",
    "            'stage': 'stage1',\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_checkpoint, r'weights\\best_stage1_arc.pth')\n",
    "        print(f\"Сохранена лучшая модель (val_acc={val_acc:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nЭтап 2: Размораживаем весь backbone (35 эпох)\")\n",
    "\n",
    "for param in model.backbone.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.backbone.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer_stage2 = optim.AdamW(\n",
    "    [\n",
    "        {'params': model.backbone.layer3.parameters(), 'lr': 2e-4},\n",
    "        {'params': model.backbone.layer4.parameters(), 'lr': 3e-4},\n",
    "        {'params': model.embedding.parameters(), 'lr': 4e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 4e-4}\n",
    "    ],\n",
    "    weight_decay=0.05\n",
    ")\n",
    "\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "EPOCHS_STAGE2 = 35\n",
    "\n",
    "scheduler_stage2 = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_stage2,\n",
    "    T_max=EPOCHS_STAGE2,  # 35 эпох плавного снижения\n",
    "    eta_min=1e-7\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE2):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{EPOCHS_STAGE2}\")\n",
    "    current_margin = (0.1 + 0.45 * (epoch / 15)) if epoch < 15  else 0.55\n",
    "    if epoch < 16:\n",
    "        model.classifier.set_margin(current_margin)\n",
    "        print(f\"  Margin m set to: {current_margin:.4f}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer_stage2, criterion, use_mixup=False)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    scheduler_stage2.step()\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val: acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_checkpoint = {\n",
    "            'stage': 'stage2',\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_checkpoint, r'weights\\best_stage2_arc.pth')\n",
    "        print(f\"Сохранена лучшая модель (val_acc={val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Ранняя остановка stage2\")\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"\\nФинальный fine-tune\")\n",
    "optimizer_finetune = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=5e-6,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "scheduler_finetune = optim.lr_scheduler.ReduceLROnPlateau(optimizer_finetune, mode='max', factor=0.5, patience=1)\n",
    "\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "EPOCHS_FINETUNE = 15\n",
    "\n",
    "current_margin = 0.5\n",
    "model.classifier.set_margin(current_margin)\n",
    "print(f\"m изменён на: {current_margin:.4f}\")\n",
    "\n",
    "for epoch in range(EPOCHS_FINETUNE):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{EPOCHS_FINETUNE}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer_finetune, criterion, use_mixup=False)\n",
    "    val_acc = validate(model, val_loader)\n",
    "    scheduler_finetune.step(val_acc)\n",
    "\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "    print(f\"Val: acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_checkpoint = {\n",
    "            'stage': 'finetune',\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(best_checkpoint, r'weights\\best_final_arc.pth')\n",
    "        print(f\"Сохранена лучшая модель (val_acc={val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Ранняя остановка finetune\")\n",
    "            break\n",
    "            \n",
    "print(f\"\\nЛучшая точность на валидации (best_val_acc): {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n   Загрузка лучшей модели\")\n",
    "best_model_path = None\n",
    "if os.path.exists(r'weights\\best_final_arc.pth'):\n",
    "    best_model_path = r'weights\\best_final_arc.pth'\n",
    "elif os.path.exists(r'weights\\best_stage2_arc.pth'):\n",
    "    best_model_path = r'weights\\best_stage2_arc.pth'\n",
    "\n",
    "checkpoint = torch.load(best_model_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Загружена лучшая модель из {best_model_path}\")\n",
    "print(f\"  Val accuracy (saved): {checkpoint.get('val_acc', 0):.4f}\")\n",
    "print(f\"  Train accuracy (saved): {checkpoint.get('train_acc', 0):.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n   Тестирование\")\n",
    "test_acc = validate(model, test_loader)\n",
    "print(f\"Точность на тестовом наборе: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n   Сохранение финальной модели\")\n",
    "final_checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'train_acc': best_checkpoint.get('train_acc', 0) if best_checkpoint is not None else 0,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'model_type': 'FaceModel_ArcFace'\n",
    "}\n",
    "torch.save(final_checkpoint, r'weights\\final_arc_model.pth')\n",
    "print(r\"Финальная модель сохранена в 'weights\\final_arc_model.pth'\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print('='*60)\n",
    "print(f\"Лучшая val accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac24a7-7997-4e48-9906-5e48efd63e04",
   "metadata": {},
   "source": [
    "В обоих случаях разбивал обучение на 3 фазы:\n",
    "1) Только embedding и classifier слои, не изменяя предобученных на imagenet весов бэкбона, чтобы хоть немного приблизиться на новых слоях к оптимальным значениям весов и они не так сильно влияли на первых эпохах на градиенты предобученных слоев\n",
    "2) Разморозил два слоя backbonе, обучал с помощью косинусного шедулера, который плавно понижает LR в зависимости от эпохи\n",
    "3) Не добавляя новых слоев пытался дообучить модель с изначально низким lr и его понижением на плато с заведомо низкой терпимостью всего в одну эпоху. На последней итерации обучения смысла от этого не оказалось, однако на прошлых давало прирост вплоть до 1%\n",
    "\n",
    "Что же касается особенностей обучения arcface. При попытках запустить обучение сразу с m = 0.5 модель не обучалась (как оказалось тогда у меня была некорректная валидация, на которой я не убирал m), вследствие чего было принято решение попробовать \"разогревать\" m, постепенно поднимая его от эпохи к эпохе и в определенный момент фиксируя на определенном значении (0.5). К концу такого обучения модель достигла вероятности правильной классификации 65% даже с условием маржи, равной 0.5. Так же при обучении с ним нельзя было использовать MixUp и Label Smoothing, как я это сделал при обучении на CE. Mixup не используется по причине того, его целью является создание плавного перехода между лицами, в то время как arcface наоборот направлен на максимизацию углового расстояния между ними. LabelSmoothing же нельзя использовать, потому что arcface целенаправленно штрафует правильную метку, что при ее размытии будет работать некорректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ff27d-ae2a-4b48-81c7-26f4c0dc592b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
